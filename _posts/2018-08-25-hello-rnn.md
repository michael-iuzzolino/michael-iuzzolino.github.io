---
layout: post
title:  "Hello, RNN!"
date:   2019-04-27
excerpt: "int main void, something something, return 0?"
image: # "/images/pic02.jpg"
---

# Hello, RNN!
The legendary post, ["The Unreasonable Effectiveness of Recurrent Neural Networks"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy introduced us to RNNs and their use as character-level language models. When I was first learning about RNNs, the jump from his very simple "hello" example to the more involved [Github code](https://medium.com/r/?url=https%3A%2F%2Fgist.github.com%2Fkarpathy%2Fd4dee566867f8291f086) was a larger leap than I had been hoping for. This post will provide a very simple PyTorch implementation for Karpathy's "hello" example. Unlike Karpathy's numpy implementation from scratch, we will be leveraging PyTorch's auto-differentiation to take care of gradients. However, we will be building RNNs (vanilla, LSTM, and GRU) from scratch rather than using PyTorch's inbuilt RNN modules.
I will only be briefly touching upon concepts here. For more detailed explanations of these concepts and the mathematics underlying recurrent models, in addition to Karpathy's post, see

- ["Understanding LSTM Networks"](https://medium.com/r/?url=http%3A%2F%2Fcolah.github.io%2Fposts%2F2015-08-Understanding-LSTMs%2F) by Christopher Olah
- ["Illustrated Guide to LSTM's and GRU's](https://medium.com/r/?url=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21): A step by step explanation" by Michael Nguyen

# Data Setup
First, we'll set up a data handler that takes an arbitrary sentence and processes the sentence for training our model.

<script src="https://gist.github.com/michael-iuzzolino/dc98554496d918286401f2943d471dfa.js"></script>

`data = DataHandler("Hello, RNN!")`

We'll instantiate our data handler class with the string, "Hello, RNN!" - our class will determine the set of unique characters in our string. We then create mappings between these characters and integer representations (e.g, 'H' → 0, 'R' → 5). We then use these index mappings to convert the characters into one-hot encodings.

<img class="custom_image" data-image-id="0*GZy1tUbL8gsiCmr4.jpg" data-width="400" data-height="300" src="https://cdn-images-1.medium.com/max/1600/0*GZy1tUbL8gsiCmr4.jpg">

Lastly, we need to generate the (X, y) data for training the model. For a character input at timestep t, the model predicts what the character at timestep t+1 ought to be. For example, if we train on the string 'hello', then feed in 'h' to the model it should predict 'e'; if we feed in 'e', it should predict 'l', and so on (see figure below).

<img class="custom_image" data-image-id="0*8LykR03GxX2TM-jj.jpeg" data-width="902" data-height="725" data-is-featured="true" src="https://cdn-images-1.medium.com/max/1600/0*8LykR03GxX2TM-jj.jpeg">

>"An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). This diagram shows the activations in the forward pass when the RNN is fed the characters "hell" as input. The output layer contains confidences the RNN assigns for the next character (vocabulary is "h,e,l,o"); We want the green numbers to be high and red numbers to be low." - [Source](https://medium.com/r/?url=http%3A%2F%2Fkarpathy.github.io%2F2015%2F05%2F21%2Frnn-effectiveness%2F)

The power of the RNN lies within modeling sequential dependencies: if we feed in an 'l', the model will learn whether the output should be another 'l' or an 'o', depending on the sequence of information processed up until the given input character.
To achieve the appropriate (X, y) data, we offset y by 1 character from X. E.g., if our string is "Hello", then X = ['H', 'e', 'l', 'l'], and y = ['e', 'l', 'l', 'o'], except that we will be using one-hot encodings rather than characters, so we will actually feed these to the model: X = [ [1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,1,0] ], y = [ [0,1,0,0], [0,0,1,0], [0,0,1,0], [0,0,0,1] ].

# RNN Model - Vanilla
We will first cover the so-called 'vanilla' RNN cell (LSTM and GRUs follow below). This cell consists of a single hidden state that is updated via tanh activation and is a function of the input at the given timestep and the hidden state's previous value.

<img class="custom_image" data-image-id="1*AyxsB0n9S0iYFdEU9qTnFQ.png" data-width="1134" data-height="736" src="https://cdn-images-1.medium.com/max/1600/1*AyxsB0n9S0iYFdEU9qTnFQ.png">

> Vanilla Cell. [Source](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)

<script src="https://gist.github.com/michael-iuzzolino/e4bd557f6632f1826b8f485263630bcd.js"></script>

We instantiate our model by giving it the number of characters in our dataset and the number of hidden units we want to comprise our hidden state. The **_init_weights** function applies [Xavier initialization](https://medium.com/r/?url=http%3A%2F%2Fproceedings.mlr.press%2Fv9%2Fglorot10a%2Fglorot10a.pdf) to the weights and ensures that **requires_grad** is True (if False, the variable will not accumulate gradients and no learning will occur).

`net = HelloRNN(num_chars=data.num_characters, num_hidden=50)`

When inheriting from nn.Module, the **forward** method is required and it acts as the primary workhorse of your class. This function is treated as a **__call__** method and is consequently called by

`output = net(data.X)`

We will also need an optimizer and a loss criterion. Here, we'll use Adam from PyTorch's built-in optimizers (torch.optim) and Cross Entropy loss.

```python
optimizer = optim.Adam(net.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()
```

Note, as of Python 3.5 we may use @ as a dedicated infix operator for matrix multiplication (see [PEP 465](https://medium.com/r/?url=https%3A%2F%2Fwww.python.org%2Fdev%2Fpeps%2Fpep-0465%2F)).

# Training
<script src="https://gist.github.com/michael-iuzzolino/34d3258691cd20f662ae439a93d02069.js"></script>


Since PyTorch's automatic differentiation takes care of the gradients for us, we have a very simple training loop. All we need to do is:
1. Zero out the gradients using the optimizer - we don't want to accumulate gradients on variables across epochs (or, when you have batches, across batches)

2. Pass X to the network and obtain the logits output

3. Compute loss from output logits and character labels via CrossEntropyLoss - this function automatically applies softmax to the outputs (see [this discussion](https://medium.com/r/?url=https%3A%2F%2Fdiscuss.pytorch.org%2Ft%2Fwhy-does-crossentropyloss-include-the-softmax-function%2F4420))

4. Call loss.backward() to compute gradients over all trainable variables (where requires_grad == True)

5. Clip parameter gradients - we use clamp in PyTorch - to avoid **exploding gradients**, a problem common to recurrent models (see ["On the difficulty of training Recurrent Neural Networks"](https://medium.com/r/?url=https%3A%2F%2Farxiv.org%2Fabs%2F1211.5063))

6. Apply computed gradients to their respective parameters. Thank you, optimizer.step()

7. Do other useful things, such as printing information to stdout, evaluating the model, logging results, etc.

# LSTMs and GRU
<img class="custom_image" data-image-id="1*yBXV9o5q7L_CvY7quJt3WQ.png" src="https://cdn-images-1.medium.com/max/1600/1*yBXV9o5q7L_CvY7quJt3WQ.png">

> [Source](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)

 In addition to the problem of exploding gradients, recurrent models may also suffer from the problem of **vanishing gradients** (again, see ["On the difficulty of training Recurrent Neural Networks"](https://medium.com/r/?url=https%3A%2F%2Farxiv.org%2Fabs%2F1211.5063)).

LSTM [[Long Short-Term Memory]](https://medium.com/r/?url=https%3A%2F%2Fwww.bioinf.jku.at%2Fpublications%2Folder%2F2604.pdf) models were developed in the late 1990's to address this issue. In short, the LSTM cell contains two 'states' - a hidden state and a cell state. Additionally, three new layers are added to the cell, each corresponding to a different gate: 1) **input gate**, 2) **forget gate**, 3) **output gate**. The parameters of these layers are trainable - they learn which information in the training data is most important for accomplishing a given task.

For example, consider sentiment analysis, where you want to analyze a sentence and determine whether the sentence is conveying 'positive' or 'negative' sentiment. In any given sentence, there are many filler words that aren't important for inferring sentiment, and they may be discarded from the cell state. Conversely, other words in the sentence may have considerable informational value and should be preserved in the cell's state. These trainable gates determine which information to keep and which to discard.

The GRU [[Gated Recurrent Unit]](https://medium.com/r/?url=https%3A%2F%2Farxiv.org%2Fabs%2F1406.1078) is a modification to the LSTM that maintains performance but reduces the number of parameters required, as it utilizes just two gates: 1) **update gate**, 2) **reset gate**. See ["Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"](https://medium.com/r/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1412.3555v1.pdf) for a comparison between cells.

These cells, along with our original 'vanilla' cell may be implemented as nn.Modules in the following way:

## Vanilla Cell
<script src="https://gist.github.com/michael-iuzzolino/09c9224fdd991f13ed484968c36eb532.js"></script>

## LSTM Cell
<script src="https://gist.github.com/michael-iuzzolino/25f685ce3d8dd78b26b8d4165180cac9.js"></script>

## GRU Cell
<script src="https://gist.github.com/michael-iuzzolino/49a921d78bf410c3f57f6858466634bd.js"></script>


And our main driver network for these cells is reconstructed as

<script src="https://gist.github.com/michael-iuzzolino/9b4560c90d32deb99d9e62aa9033aa59.js"></script>

The full, working code can be found here:

<script src="https://gist.github.com/michael-iuzzolino/9b4560c90d32deb99d9e62aa9033aa59.js"></script>


That's all for now, and I hope that you have found this useful in some capacity.
